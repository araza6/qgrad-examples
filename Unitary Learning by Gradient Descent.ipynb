{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and background\n",
    "\n",
    "In this tutorial, we aim to learn unitary matrices using gradient descent. The tutorial reproduces [Lloyd et al.](https://arxiv.org/abs/1901.03431) and [Bobak et al.](https://arxiv.org/abs/2001.11897), and follows similar formalism as introduced in the two papers.\n",
    "\n",
    "For a target unitary matrix, $U$, we intend to find optimal parameter vectors for the parameterized unitary $U(\\vec{t}, \\vec{\\tau})$, such that $U(\\vec{t}, \\vec{\\tau})$ approximates $U$ as closely as possible. Here, \n",
    "\n",
    "\\begin{equation}\\label{decomp}\n",
    "    U(\\vec{t}, \\vec{\\tau}) = e^{-iB\\tau_{N}}e^{-iAt_{N}} ... e^{-iB\\tau_{1}}e^{-iAt_{1}}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\vec{t}$ and $\\vec{\\tau}$ are paramter vectors of size $N$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import tenpy \n",
    "\n",
    "from matplotlib import cm\n",
    "from qutip import *\n",
    "from scipy.stats import unitary_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "- We are going to be using random input kets as inputs, call them `ket_input` \n",
    "- Output kets are defined as `ket_output` = $U(\\vec{t}, \\vec{\\tau})*|ket\\_input>$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(m, d):\n",
    "    r\"\"\"Prepares a dataset of input and output kets to be used for training\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        m (int): Number of data points to be used for training\n",
    "        d (int): Dimension of a (square) unitary matrix to be approximated\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        data_points (tuple): tuple of lists containing (numpy arrays of) input and output kets respectively.\n",
    "    \"\"\"\n",
    "    tar_unitr = unitary_group.rvs(d)  # Fixed random d-dimensional target unitary matrix that we want to learn\n",
    "    ket_input = []\n",
    "    ket_output = [] \n",
    "    for i in range(m):\n",
    "        ket_input.append(np.random.rand(d,1))\n",
    "        ket_output.append(np.matmul(tar_unitr, ket_input[i]))  #Output data -- action of unitary on a ket states\n",
    "    \n",
    "    return (ket_input, ket_output)\n",
    "\n",
    "m = 1000 # number of training data points\n",
    "d = 2 #dimension of unitary \n",
    "N = 1 #size of parameter vectors tau and t\n",
    " \n",
    "res = make_dataset(m, d)\n",
    "ket_input, ket_output = res[0], res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe for making $U(\\vec{t}, \\vec{\\tau})$\n",
    "\n",
    "We make $U(\\vec{t}, \\vec{\\tau})$ by repeated application of $e^{-iB\\tau_{k}}e^{-iAt_{k}}$ at k-th step. We multiply $e^{-iB\\tau_{k}}e^{-iAt_{k}}$ in a [QAOA](https://arxiv.org/abs/1411.4028) like fashion $N$ times, where N is the dimension of $\\vec{t}$ and $\\vec{\\tau}$. Higher N $\\rightarrow$ better approximation.\n",
    "\n",
    "In [Lloyd et al.](https://arxiv.org/abs/1901.03431) and [Bobak et al.](https://arxiv.org/abs/2001.11897), matrices $A$ and $B$ are chosen from a Gaussian Unitary Ensemble (GUE). We use `tenpy` to sample $A$ and $B$ from GUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tenpy.linalg.random_matrix.GUE((d,d)) # tenpy for sampling A and B from GUE\n",
    "B = tenpy.linalg.random_matrix.GUE((d,d)) \n",
    "\n",
    "def make_unitary(N, params):\n",
    "    r\"\"\"Retruns a paramterized unitary matrix \n",
    "    \n",
    "    : math:: \\begin{equation}\\label{decomp}\n",
    "                U(\\vec{t}, \\vec{\\tau}) = e^{-iB\\tau_{N}}e^{-iAt_{N}} ... e^{-iB\\tau_{1}}e^{-iAt_{1}}\n",
    "             \\end{equation}\n",
    "             \n",
    "    Args:\n",
    "    ----\n",
    "        N (int): Size of the parameter vectors, :math:`\\tau` and :math:`\\t`\n",
    "        params (:obj:`np.ndarray`): parameter vector of size :math:`2 * N` where the first half parameters are  \n",
    "                                   :math:`\\vec{t}` params and the second half encodes \\vec{\\tau}) parameters.\n",
    "                                   \n",
    "    Returns:\n",
    "        unitary (:obj:`np.ndarray`): numpy array representation of paramterized unitary matrix \n",
    "    \"\"\"\n",
    "    unitary = np.eye(d)\n",
    "    for i in range (N): \n",
    "        unitary = np.matmul(np.matmul(expm(-1j*B*params[i+N][0]),expm(-1j*A*params[i][0])), unitary)\n",
    "    \n",
    "    return unitary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria for learnability -- the cost function\n",
    "The cost function formulation of gradient descent learning is defined by the authors is as follows:\n",
    "\n",
    "$\\begin{equation} \\label{err_ps}\n",
    "        E = 1 - (\\frac{1}{M})\\sum_{l} \\langle \\psi_{l}|U^{\\dagger} U(\\vec{t},\\vec{\\tau})|\\psi_{l}\\rangle\n",
    "\\end{equation}$, \n",
    "\n",
    "where $ |\\psi_{l}>$ is the training (or testing) data points -- in this case, kets. We implement the same formulation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(params, inputs, outputs): # here params is the optimized unitary matrix parameters at each step \n",
    "    loss = 0.0\n",
    "    for k in range(m): \n",
    "        pred = np.matmul(make_unitary(params), inputs[k]) # prediction wth parametrized unitary\n",
    "        loss += np.absolute(np.real(np.matmul(outputs[k].conjugate().T, pred)))\n",
    "        # TODO check real and abs in loss above since  \n",
    "        # it's not in the original paper\n",
    "    return 1 - (1 / m) * loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentation of the cost function\n",
    "\n",
    "Gradient descent is a first order method, so one definitely needs to take the derivative of the cost function. And here is where things get a bit messy. The gradient of above error term or the cost function is \n",
    "\n",
    "$ \n",
    "\\begin{equation}\n",
    "    \\frac{\\partial}{\\partial t_{k}}E(\\vec{t},\\vec{\\tau}) = -\\frac{1}{M}\\sum_{l} \\langle \\psi_{l}|U^{\\dagger}[e^{-iAt_{N}}e^{-iB\\tau_{N}} ... (-iA)e^{-iAt_{k}}e^{-iB\\tau_{k}} ... e^{-iAt_{1}}e^{-iB\\tau_{1}}]|\\psi_{l}\\rangle\n",
    "\\end{equation}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
