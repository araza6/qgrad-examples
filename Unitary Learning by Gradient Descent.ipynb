{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and background\n",
    "\n",
    "In this tutorial, we aim to learn unitary matrices using gradient descent. The tutorial reproduces [Lloyd et al.](https://arxiv.org/abs/1901.03431) and [Bobak et al.](https://arxiv.org/abs/2001.11897), and follows similar formalism as introduced in the two papers.\n",
    "\n",
    "For a target unitary matrix, $U$, we intend to find optimal parameter vectors for the parameterized unitary $U(\\vec{t}, \\vec{\\tau})$, such that $U(\\vec{t}, \\vec{\\tau})$ approximates $U$ as closely as possible. Here, \n",
    "\n",
    "\\begin{equation}\\label{decomp}\n",
    "    U(\\vec{t}, \\vec{\\tau}) = e^{-iB\\tau_{N}}e^{-iAt_{N}} ... e^{-iB\\tau_{1}}e^{-iAt_{1}}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\vec{t}$ and $\\vec{\\tau}$ are paramter vectors of size $N$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import tenpy \n",
    "from matplotlib import cm\n",
    "from qutip import fidelity, Qobj, rand_ket\n",
    "from scipy.stats import unitary_group\n",
    "from scipy.linalg import expm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "- We are going to be using random input kets as inputs, call them `ket_input` \n",
    "- Output kets are defined as `ket_output` = $U(\\vec{t}, \\vec{\\tau})*|ket\\_input>$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(m, d):\n",
    "    r\"\"\"Prepares a dataset of input and output kets to be used for training.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        m (int): Number of data points to be used for training\n",
    "        d (int): Dimension of a (square) unitary matrix to be approximated\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        data_points (tuple): tuple of lists containing (numpy arrays of) input and output kets respectively.\n",
    "    \"\"\"\n",
    "    tar_unitr = unitary_group.rvs(d)  # Fixed random d-dimensional target unitary matrix that we want to learn\n",
    "    ket_input = []\n",
    "    ket_output = [] \n",
    "    for i in range(m):\n",
    "        ket_input.append(rand_ket(d).full())\n",
    "        ket_output.append(np.matmul(tar_unitr, ket_input[i]))  #Output data -- action of unitary on a ket states\n",
    "    \n",
    "    return (ket_input, ket_output)\n",
    "\n",
    "m = 1000 # number of training data points\n",
    "d = 2 #dimension of unitary \n",
    "N = 5 #size of parameter vectors tau and t\n",
    " \n",
    "res = make_dataset(m, d)\n",
    "ket_input, ket_output = res[0], res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe for making $U(\\vec{t}, \\vec{\\tau})$\n",
    "\n",
    "We make $U(\\vec{t}, \\vec{\\tau})$ by repeated application of $e^{-iB\\tau_{k}}e^{-iAt_{k}}$ at k-th step. We multiply $e^{-iB\\tau_{k}}e^{-iAt_{k}}$ in a [QAOA](https://arxiv.org/abs/1411.4028) like fashion $N$ times, where N is the dimension of $\\vec{t}$ and $\\vec{\\tau}$. Higher N $\\rightarrow$ better approximation.\n",
    "\n",
    "In [Lloyd et al.](https://arxiv.org/abs/1901.03431) and [Bobak et al.](https://arxiv.org/abs/2001.11897), matrices $A$ and $B$ are chosen from a Gaussian Unitary Ensemble (GUE). We use `tenpy` to sample $A$ and $B$ from GUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tenpy.linalg.random_matrix.GUE((d,d)) # tenpy for sampling A and B from GUE\n",
    "B = tenpy.linalg.random_matrix.GUE((d,d)) \n",
    "\n",
    "def make_unitary(N, params):\n",
    "    r\"\"\"Retruns a paramterized unitary matrix.\n",
    "    \n",
    "    : math:: \\begin{equation}\\label{decomp}\n",
    "                U(\\vec{t}, \\vec{\\tau}) = e^{-iB\\tau_{N}}e^{-iAt_{N}} ... e^{-iB\\tau_{1}}e^{-iAt_{1}}\n",
    "             \\end{equation}\n",
    "             \n",
    "    Args:\n",
    "    ----\n",
    "        N (int): Size of the parameter vectors, :math:`\\tau` and :math:`\\t`\n",
    "        params (:obj:`np.ndarray`): parameter vector of size :math:`2 * N` where the first half parameters are  \n",
    "                                   :math:`\\vec{t}` params and the second half encodes \\vec{\\tau}) parameters.\n",
    "                                   \n",
    "    Returns:\n",
    "        unitary (:obj:`np.ndarray`): numpy array representation of paramterized unitary matrix \n",
    "    \"\"\"\n",
    "    unitary = np.eye(d)\n",
    "    for i in range (N): \n",
    "        unitary = np.matmul(np.matmul(expm(-1j*B*params[i+N][0]),expm(-1j*A*params[i][0])), unitary)\n",
    "    \n",
    "    return unitary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria for learnability -- the cost function\n",
    "The cost function formulation of gradient descent learning is defined by the authors is as follows:\n",
    "\n",
    "$\\begin{equation} \\label{err_ps}\n",
    "        E = 1 - (\\frac{1}{M})\\sum_{l} \\langle \\psi_{l}|U^{\\dagger} U(\\vec{t},\\vec{\\tau})|\\psi_{l}\\rangle\n",
    "\\end{equation}$, \n",
    "\n",
    "where $ |\\psi_{l}>$ is the training (or testing) data points -- in this case, kets. We implement the same formulation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(params, inputs, outputs):\n",
    "    r\"\"\"Calculates the cost/error on the whole training datat set.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        params: parameters :math:`\\t` and :math:`\\tau` in :math:`U^{\\dagger} U(\\vec{t},\\vec{\\tau})`\n",
    "        inputs: input kets :math:`|\\psi_{l}>` in the dataset \n",
    "        outputs: output kets :math:`U(\\vec{t}, \\vec{\\tau})*|ket\\_input>` in the dataset\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "        cost (float): cost (evaluated on the enitre dataset) of parametrizing \n",
    "                     :math:`\\tau` in :math:`U^{\\dagger} U(\\vec{t},\\vec{\\tau})` with `params`                  \n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    for k in range(m): \n",
    "        pred = np.matmul(make_unitary(N, params), inputs[k]) #prediction wth parametrized unitary\n",
    "        loss += np.absolute(np.real(np.matmul(outputs[k].conjugate().T, pred)))\n",
    "        # TODO check real and abs in loss above since   \n",
    "        # it's not in the original paper\n",
    "    return 1 - (1 / m) * loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentation of the cost function\n",
    "\n",
    "Gradient descent is a first order method, so one definitely needs to take the derivative of the cost function. Analytically, the gradient of above error term, or the cost function, is \n",
    "\n",
    "$ \n",
    "\\begin{equation}\n",
    "    \\frac{\\partial}{\\partial \\tau_{k}}E(\\vec{t},\\vec{\\tau}) = -\\frac{1}{M}\\sum_{l} \\langle \\psi_{l}|U^{\\dagger}[e^{-iAt_{N}}e^{-iB\\tau_{N}} ... (-iB)e^{-iB\\tau_{k}}e^{-iAt_{k}} ... e^{-iB\\tau_{1}}e^{-iAt_{1}}]|\\psi_{l}\\rangle\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "# **TODO Shahnawaz to recheck the eq** above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO this function can be made elegant\n",
    "#TODO test the derivative func\n",
    "def der_cost(params, inputs, outputs):\n",
    "    r\"\"\"Returns the derivative of the cost function.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        params: parameters :math:`\\t` and :math:`\\tau` in :math:`U^{\\dagger} U(\\vec{t},\\vec{\\tau})`\n",
    "        inputs: input kets :math:`|\\psi_{l}>` in the dataset \n",
    "        outputs: output kets :math:`U(\\vec{t}, \\vec{\\tau})*|ket\\_input>` in the dataset\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        der (obj:`np.array`): Cost function value w.r.t each of the 2 * N parameters    \n",
    "    \"\"\"\n",
    "    der_params = []\n",
    "    for i in range(2 * N):\n",
    "            dU = np.eye(d)\n",
    "            if i < N:\n",
    "                for k in range(N):\n",
    "                    if k == i:\n",
    "                        term_t = np.matmul(expm(-1j * B * params[k + N][0]), \n",
    "                                           np.matmul(-1j * A, expm(-1j * A * params[k][0])))\n",
    "                        dU = np.matmul(term_t, dU)\n",
    "                    elif k != i :\n",
    "                        dU = np.matmul(np.matmul(expm(-1j * B * params[k + N][0]), \n",
    "                                                 expm(-1j * A * params[k][0])), dU)\n",
    "\n",
    "                \n",
    "                sum_t = 0\n",
    "                for j in range(m):\n",
    "                    pred_t = np.matmul(dU, inputs[j])\n",
    "                    sum_t += np.real(np.matmul(outputs[j].conjugate().T,pred_t))\n",
    "                \n",
    "                der_params.append((-1/m) * sum_t)\n",
    "           \n",
    "            elif i >= N: \n",
    "                for j, k in enumerate(range(N, 2 * N)):\n",
    "                    if k == i:\n",
    "                        term_tau = np.matmul(np.matmul(-1j * B, expm(-1j * B * params[j + N][0])), \n",
    "                                                       expm(-1j * A * params[j][0]))\n",
    "                        dU = np.matmul(term_t, dU)\n",
    "                    elif k != i:\n",
    "                        dU = np.matmul(np.matmul(expm(-1j * B * params[j + N][0]),\n",
    "                                                 expm(-1j * A * params[j][0])), dU)\n",
    "\n",
    "                sum_tau = 0\n",
    "                for j in range(m):\n",
    "                    pred_tau = np.matmul(dU, inputs[j])\n",
    "                    sum_tau += np.real(np.matmul(outputs[j].conjugate().T, pred_tau))\n",
    "                \n",
    "                der_params.append((-1/m) * sum_tau)\n",
    "                \n",
    "                        \n",
    "    der_params = np.asarray(der_params).reshape(2 * N, 1)\n",
    "    return der_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metric -- fidelity\n",
    "\n",
    "While cost is a valid metric to judge the learnability. We introduce another (commonly used) metric, fidelty between the predicted and the output (label) states as a sanity check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_score(params, x, y):\n",
    "    \"\"\"Calculates the avergage fidelity between the predicted and output kets for a given \n",
    "       on the whole dataset.\n",
    "       \n",
    "       Args:\n",
    "       ----\n",
    "           params: parameters :math:`\\t` and :math:`\\tau` in :math:`U^{\\dagger} U(\\vec{t},\\vec{\\tau})`\n",
    "           x: input kets :math:`|\\psi_{l}>` in the dataset \n",
    "           y: output kets :math:`U(\\vec{t}, \\vec{\\tau})*|ket\\_input>` in the dataset\n",
    "           \n",
    "       Returns:\n",
    "       -------\n",
    "           fidel (float): fidelity between :math:`U(\\vec{t}, \\vec{\\tau})*|ket\\_input>` and\n",
    "                          the output (label) kets for parametrs :math:`\\vec{t}, \\vec{\\tau}`\n",
    "                          averaged over the entire training set.\n",
    "       \"\"\"\n",
    "    fidel = 0\n",
    "    for i in range(m):\n",
    "        pred = np.matmul(make_unitary(N, params), x[i])\n",
    "        step_fidel = fidelity(Qobj(pred), Qobj(y[i]))\n",
    "        fidel += step_fidel\n",
    "        \n",
    "        \n",
    "    return fidel/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Implementation\n",
    "\n",
    "We implement gradient descent based on \n",
    "\n",
    "$\\begin{equation} \\label{gd}\n",
    "    t_{k} = t_{k} - \\alpha \\frac{\\partial}{\\partial t_{k}}E(\\vec{t},\\vec{\\tau})\n",
    "\\end{equation}$\n",
    "\n",
    "for a single parameter $t_{k}$, where $\\alpha$ is the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.000000 | Loss: 0.631956 | Fidelity: 0.845073\n",
      "Epoch: 2.000000 | Loss: 0.635454 | Fidelity: 0.851093\n",
      "Epoch: 3.000000 | Loss: 0.606605 | Fidelity: 0.855212\n",
      "Epoch: 4.000000 | Loss: 0.505758 | Fidelity: 0.859521\n",
      "Epoch: 5.000000 | Loss: 0.386299 | Fidelity: 0.866254\n",
      "Epoch: 6.000000 | Loss: 0.292801 | Fidelity: 0.876676\n",
      "Epoch: 7.000000 | Loss: 0.222790 | Fidelity: 0.890478\n",
      "Epoch: 8.000000 | Loss: 0.172115 | Fidelity: 0.906028\n",
      "Epoch: 9.000000 | Loss: 0.136964 | Fidelity: 0.921078\n",
      "Epoch: 10.000000 | Loss: 0.114926 | Fidelity: 0.933388\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "alpha = 0.01\n",
    "tol = 1e-7\n",
    "diff = 1\n",
    "max_iters = 10\n",
    "iters = 0\n",
    "loss_hist = []\n",
    "fidel_hist = []\n",
    "params_hist = []\n",
    "weights = np.random.rand(2 * N, 1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    iters = 0\n",
    "    tol = 1e-7\n",
    "    diff = 1\n",
    "    while np.all(diff > tol) and iters < max_iters:\n",
    "        prev_weights = weights\n",
    "        weights = weights - alpha * (der_cost(prev_weights, ket_input, ket_output))\n",
    "        iters += 1\n",
    "        diff = np.absolute(weights - prev_weights)\n",
    "    loss = cost(weights, ket_input, ket_output).item() # convert numpy (1,1) array to native python float\n",
    "    avg_fidel = test_score(weights, ket_input, ket_output)\n",
    "    progress = [epoch+1, loss, avg_fidel]\n",
    "    loss_hist.append(loss)\n",
    "    fidel_hist.append(avg_fidel)\n",
    "    params_hist.append(weights)\n",
    "    #if ((epoch) % 10 == 1):\n",
    "    print(\"Epoch: {:2f} | Loss: {:3f} | Fidelity: {:3f}\".format(*np.asarray(progress)))\n",
    "    \n",
    "opt_params = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
